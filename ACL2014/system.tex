
\section{Problem Statement}
We start with a corpus of user queries being represented as vectors
over some domain (e.g. URLs); similarity between queries is to be measured using cosine between the corresponding
vectors. We are also given a set of queries $Q$. 
We are interested in developing a batch process to return a \emph{small}
set of candidate neighbors for each query $q \in Q$ such that the following
are satisfied: 1) the similarity of any returned candidate to the query $q$ is larger than a user-specified 
similarity threshold $\tau$; 2) the set of candidates returned has a large recall. 
Note that, although we return an approximate set of near neighbors, our requirement forces us to 
return a candidate set that has $100\%$ precision. Our aim is to achieve a large recall, while using
a scalable efficient algorithm.

The exact brute force algorithm to solve the above problem would be to compute
similarities between each $q\in Q$ and all queries in the corpus, and return 
all pairs that have similarities higher than the threshold $\tau$. Computing similarities
for all pairs is computationally infeasible on a single computer even if the size of $Q$ is of the order of few thousands
and the corpus size is hundreds of millions. Even in a distributed setting such as Hadoop, the 
resulting communication needed between machines makes this strategy impractical. 

Our aim is to empirically study a set of locality sensitive hashing techniques
that enable us to return a set of candidate neighbors while performing
a much smaller (theoretically \emph{sublinear}) set of comparisons. 


The motivated behind this research is to find nearest neighbors by doing a 
\emph{small} number of comparisons (sublinear in dataset size), instead of brute force linear search. 
In addition to \emph{small} number of comparisons, we also want to 
return a set of neighbor candidates with a $100\%$ precision and a large recall. 
The method we propose meet all these criterions. We do this by exploiting 
existing research in \lshf (a.k.a., \lsh) and propose their novel variants. 
In particular, we develop the framework of  \lsh algorithms on a distributed 
system (e.g. Hadoop) to take advantage of its computing efficiency.



\section{Approach}
\label{sec:approach}
We describe a distributed Hadoop framework based on \lshf (\lsh). 
First, we present vanilla \lsh algorithm based on the seminal research of Andoni and Indyk  \shortcite{Andoni08CACM}. To best of our knowledge, this is the first paper that applies this algorithm to NLP applications. 

This algorithm improves the existing research in \lsh and \plebf \cite{Indyk98STOC,Charikar02STOC}. 
\plebf was applied for noun clustering \cite{ravichandran05} and speech tasks \cite {JansenASRU11,JansenIS12}. 
Recent prior work on new variants of \plebf  \cite{goyal12Flag} for distributional similarity is a special case of Andoni and Indyk's \lsh algorithm. 

Next, we present four new variants of vanilla \lsh algorithm motivated by the research on Multi-Probe LSH \cite{LvVLDB07}. 
A significant drawback of vanilla \lsh is the requirement for a large number of hash tables in order to achieve good recall 
in finding nearest neighbors. 
The goal of  \mblshf is to get significantly better recall than the vanilla \lsh by using 
the same number of hash tables. 
%The main idea behind \mblshf is to look up multiple buckets within a table that have a high probability 
%of containing the nearest neighbors of a query. We present a high-level idea behind the \mblshf algorithm; 
%for more details, the reader is referred to \cite{LvVLDB07}.


\subsection{Vanilla \lsh}
\label{sec:vlsh}
The \lsh algorithm relies on the existence of an \lsh family. Let $H$ be a family of hash functions mapping 
 $\mathbb{R}^D$ to some universe S. For any two query terms $p$, $q$; we chose $h\in H$ uniformly at random; 
 and analyze the probability that $h(p) = h(q)$.  The family $H$ is called a \lsh family if 
 it satisfies the following conditions:

\begin{enumerate}
\item $d(p,q) \leq R$, then $Pr[h(p)=h(q)] \geq P_1$ 
\item $d(p,q) \geq cR$, then $Pr[h(p)=h(q)] \leq P_2$
\end{enumerate}

In above conditions, $d$ is a distance function\footnote{In this paper, we use cosine distance.}, 
a distance threshold $R>0$ and an approximation factor $c>1$.   

A family is generally interesting when  $P_1>P_2$. 
However, the difference between $P_1$ and $P_2$ can be very small. 
Given a family $H$ of hash functions with parameters ($R, cR, P_1,P_2$), 
\lsh algorithm is devised by amplifying the gap between 
the two probabilities $P_1$ and $P_2$ by concatenating several functions. 
In particular, \lsh algorithm concatenates $K$ hash functions: 
$g(q)=(h_1(q),h_2(q),\dots,h_K(q))$. 
The larger value of $K$ leads to larger 
gap between probabilites of collision between close queries and far queries; 
the probabilities are $P_1^{K}$ and $P_2^{K}$ respectively. 
The advantage of this amplification is that now the algorithm is more precise meaning 
the probability of similar queries falling in same bucket is big; 
and dis-similar queries falling in same bucket is small.  

In addition to the \lsh algorithm being precise, it needs to have large recall. 
Instead of generating one set of hash table with index $g(q)=(h_1(q),h_2(q),\dots,h_K(q))$, 
\lsh algorithm generates $L$ tables, $g_j(q)=(h_{1,j}(q),h_{2,j}(q),\dots,h_{K,j}(q)))$; $\forall 1 \leq j \leq  L$. 

%Recall = $1-(1-P_1^{K})^{L}$

\subsection{\lsh for Cosine Similarity}
\label{subsec:vlsh:cosine}
In this paper, we are interested in cosine similarity, and use the \lsh family defined by Charikar \shortcite{Charikar02STOC}. 
For two queries; $p,q \in \mathbb{R}^D$, the cosine similarity is $\left( \frac{p.q}{ \| p \| \| q \| }\right)$. 
The \lsh functions for cosine similarity is defined using sign($\alpha.p$); where $\alpha \in \mathbb{R}^D$ is a random vector. 
Negative sign is represented as zero and positive sign as one; hence indexes of buckets in hash tables are $K$-sized bit signatures.  
To generate $\alpha$, we exploit the prior work on 
generating random projections \cite{Achlioptas03,LiH06} and 
use hash functions trick to implicitly map features ($\mathbb{R}^D$) to a set of \{-1,+1\}.
\footnote{Note, we represent our queries ($p$ and $q$) using only non-zero features.}  
This hashing trick is important and useful as we do not need to explicitly store the 
huge random projection matrix of size $D \times K \times L$.  

%To solve the above problem, we propose a distributed Hadoop framework based on Locality Sensitive Hashing. 
%Our framework has three main steps: 
%i) Generate $256$ bit signature for each query.  
%ii) Use the $256$ bit signature for each query to generate $L$ tables for each table with $K$ bit signature. 
%Next, generate query pairs which fall in the same table.
%iii) Compute cosine similarity between query terms which fall in the same table and return the query pairs  
%with similarity $\geq\tau$.  

Vanilla \lsh has two main steps: preprocessing the data and querying the database.  
The pseudo code written for vanilla \lsh is summarized in . 

In preprocessing step, the algorithm takes input as $N$ queries with an associated feature vectors. In our setting, 
feature vector is very high dimensional and sparse; that is the clicked webpages (urls) for a given query term. 
During preprocessing, first we generate $L\times K$ hash functions. 
Next, we hash all queries using these hash functions to build $L$ hash tables 
with each hash table containing at most $2^K$ buckets. 
There are at most $2^K$ buckets because index of each bucket: $g_j$ is $K$-widith bit index. 
Each query term is placed in their respective buckets for all  $L$ hash tables.

In querying step, for input $M$ test queries, we retrieve all the query 
terms which appear in the buckets associated with $M$ test queries. 
Next, compute cosine similarity between each of the retrieved terms 
and the input test queries. Return all those queries as neighbors 
which are within a similarity threshold ($\tau$). 

In this research, we have implemented the above algorithm within the distributed setting (Hadoop).   
In our experiments, we show that the algorithm scales to hundreds of millions of queries (see Section \ref{subsec:eval:vanillaLSH}).  

%in Algorithm \ref{alg:vanilla-LSH}.

\fbox{
\begin{minipage}{2.5in}
{\bf Preprocessing:} Input is $N$ queries with their respective feature vectors \\
\begin{itemize}
\item Select $L$ functions $g_j$, $j=1,2,\dots,L$, setting  $g_j(q)=(h_{1,j}(q),h_{2,j}(q),\dots,h_{K,j}(q)))$, where $h_{1,j}(q),h_{2,j}(q),\dots,h_{K,j}(q)))$ are chosen at random from the \lsh family.
\item Construct $L$ hash tables, $\forall 1 \leq j \leq  L$. All queries with similar $g_j$ functions ($\forall 1 \leq j \leq  L$) are placed in the same bucket.    
\end{itemize}

{\bf Query:} Input is queries ($q=1,2,\dots,M$); $M$ is the number of test queries \\
\begin{itemize}
\item For each $j=1,2,\dots,L$
\begin{itemize}
\item Retrieve all the queries from the bucket with $g_j(q)$ function as the index of the bucket 
\item Compute cosine similarity between query q and all the retrieved queries. Return all the queries which have similarity affinity within a similarity threshold ($\tau$). 
\end{itemize}
\end{itemize}
\end{minipage}
}

\subsection{Reusing Hash Functions}
\label{subsec:resuseHash}
In Section \ref{subsec:vlsh:cosine}, we showed that vanilla \lsh requires $L\times K$ hash functions. 
However generating hash functions is computationally expensive as 
it takes time to read all features and 
evaluate hash functions over all those features to generate a single bit. 
To minimize the number of hash functions computations, 
we use a trick from Andoni and Indyk \shortcite{Andoni08CACM} 
in which hash functions are reused to generate $L$ tables. $K$ is assumed to be even and $R \approx	\sqrt L$. 
We generate $f(q)=(h_1(q),h_2(q),\dots,h_{K/2}(q)))$ of length $k/2$. 
Next, we define $g(q)=(f_a,f_b)$, where $1\leq a < b \leq R$. This scheme generates $L= \frac{R (R -1)}{2}$. 
This scheme requires O($K  \sqrt L$) hash functions, instead of O($KL$).    
For rest of this paper, we use the above trick to generate $L$ hash tables with $K$-widith bit bucket-ids. 
\begin{table}
\centering
{
\small \addtolength{\tabcolsep}{-4.5pt}
\begin{tabular}{ll}
\hline
\hline
 \textbf{Symbol} &  \textbf{Description} \\
\hline 
$N$ & \# of query terms \\
$D$ & \# of features i.e. all clicked unique urls \\
\hline
\multirow{3}{*}{$K$} & \# of hash functions concatenated together \\ 
   & $g(q)=(h_1(q),h_2(q),\dots,h_k(q)))$ \\ 
	 & to generate the index of a table \\
\hline
 \multirow{2}{*}{$L$} & \# of tables generated independently \\ 
	  &  with $g_j(q)$ index, $\forall 1 \leq j \leq  L$  \\
\hline
$F$ & \# of bits flipped, $\forall 1 \leq j \leq  L$   \\
$\tau$ & $\tau$ threshold \\
Recall & fraction of similar candidates retrieved \\
Comparisons & Avg \# of pairwise comparisons per query  \\
\hline 
\end{tabular}
\caption{\footnotesize{Major Notations}}
}
\label{tab:notation}
\end{table}


\subsection{Multi Probe \lsh}
\label{sec:mblsh}
As we discussed in Section \ref{subsec:resuseHash} that generating hash functions can be computationally expensive.  
That is exactly one of the significant drawbacks of vanilla \lsh as it requires a large number of hash tables (large $L$) 
in order to achieve good recall in finding nearest neighbors.
Here, We describe four new variants of vanilla \lsh algorithm motivated by the research on \mblshf \cite{LvVLDB07}. 
The goal of  \mblshf is to get significantly better recall than the vanilla \lsh by using 
the same number of hash tables. The main concept behind \mblshf is to look 
up multiple buckets within a table that have a high probability of containing the nearest neighbors of a query. 
Note, eventhough vanila \lsh and \mblshf both have same number of hash tables ($L$); 
\mblshf requires more number of probes as it searches in multiple buckets within a table. 
The main advantage of searching in multiple buckets over generating more number of tables 
is that it takes more memory and time to generate more tables in pre-processing. 


The original \mblshf algorithm is developed for Euclidean distance. 
If interested in details about \mblshf algorithm for Euclidean distance, the reader is referred to \cite{LvVLDB07}. 
Here, we propose four variants of \mblshf for cosine similarity:
\begin{itemize}
\item {\bf \rflipq:} The first variant is our baseline: first we compute the initial \lsh of a query q, which gives the $L$ bucket ids.  
Next, we create alternate bucket ids by taking each of the $L$ bucket ids and then creating alternate candidate buckets by flipping a 
set of coordinates randomly in the \lsh of just the query.  
\item {\bf \rflipb:} The second variant is another baseline similar to the previous one. Instead of just flipping the bits for only the query, 
here we flip bits for both the query and all the queries in the database. 
\item {\bf \dflipq:} The third variant is intelligent version of first variant. Instead of randomly flipping some coordinates, 
we devisely select a set of coordinates based on the distance of query term from the random hyperplane (hash function) 
that was used in to create this coordinate. 
The distance of query term from the random hyperplane is the absolute value which we get before applying the sign function on it (see Section \ref{subsec:vlsh:cosine}). 
\item {\bf \dflipb:} The fourth version is similar to third one, however 
here we flip bits for both the query and the queries in the database (intelligent version of second baseline). 
\end{itemize}


%we have experimented with is the following variant of Multi-probe \lsh:  first we compute the initial \lsh of a query q, which gives the $L$ bucket ids. 

%We have experimented with two different methods to choose which coordinate to flip: randomly choosing a specified number of coordinates or choosing the set of coordinates based on the distance of q from the random hyperplane that was used in to create this coordinate.  
%{\rflip \lsh}
%Random Flips

%We experimented with flipping the bits in just the query and both the query and the database of all queries.  
%1. Generate less number of hash functions
%2. Have to compute similarity over less number of data pairs

%Advantages of Flipping bits compared to increasing L:
%1. Increasing $L$ means generating more random projection bits that is both time and memory intensive.   
%Note, we are already using the trick mentioned in Original \lsh to decrease
%the number of tables. Our implementation requires O(sqrt(L)) tables.
%2. Probing in more tables for vanilla \lsh, hence we need to read data from the disk (more disk read operations.). While Flipping bits, data is
%already in memory, and the given data needs to be flipped.
%3. Disadvantage of flipping is we need to store random projection distances to perform flipping (which takes space).


%\dflip \lsh
%Based on random projection distance 
