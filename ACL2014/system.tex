
\section{Problem Statement}
We start with a corpus of user queries being represented as vectors
over some domain (e.g. URLs); similarity between queries is to be measured using cosine between the corresponding
vectors. We are also given a set of queries $Q$. 
We are interested in developing a batch process to return a \emph{small}
set of candidate neighbors for each query $q \in Q$ such that the following
are satisfied: 1) the similarity of any returned candidate to the query $q$ is larger than a user-specified 
similarity threshold $\tau$; 2) the set of candidates returned has a large recall. 
Note that, although we return an approximate set of near neighbors, our requirement forces us to 
return a candidate set that has $100\%$ precision. Our aim is to achieve a large recall, while using
a scalable efficient algorithm.

The exact brute force algorithm to solve the above problem would be to compute
similarities between each $q\in Q$ and all queries in the corpus, and return 
all pairs that have similarities higher than the threshold $\tau$. Computing similarities
for all pairs is computationally infeasible on a single computer even if the size of $Q$ is of the order of few thousands
and the corpus size is hundreds of millions. Even in a distributed setting such as Hadoop, the 
resulting communication needed between machines makes this strategy impractical. 

Our aim is to empirically study a set of locality sensitive hashing techniques
that enable us to return a set of candidate neighbors while performing
a much smaller (theoretically \emph{sublinear}) set of comparisons. 
In order to tackle this scalability problem, we explore the 
combination of distributed computation using a map-reduce platform (Hadoop) as well as
locality sensitive hashing (\lsh) algorithms. 
We explore a few commonly known variants of \lsh and suggest a few novel variants
that are suitable to the map-reduce platform. 
The methods that we propose meets the practical requirements of a real life 
search engine backend, and demonstrates how to use locality sensitive hashing
on a distributed platform. 

% The motivated behind this research is to find nearest neighbors by doing a 
% \emph{small} number of comparisons (sublinear in dataset size), instead of brute force linear search. 
% In addition to \emph{small} number of comparisons, we also want to 
% return a set of neighbor candidates with a $100\%$ precision and a large recall. 
% The method we propose meet all these criterions. We do this by exploiting 
% existing research in \lshf (a.k.a., \lsh) and propose their novel variants. 
% In particular, we develop the framework of  \lsh algorithms on a distributed 
% system (e.g. Hadoop) to take advantage of its computing efficiency.



\section{Approach}
\label{sec:approach}
We describe a distributed \lshf framework based on map-reduce. 
First, we present vanilla \lsh algorithm based on the seminal work of Andoni and Indyk  \shortcite{Andoni08CACM}. 
To best of our knowledge, this is the first paper that applies this variant of locality sensitive hashing algorithms to NLP applications. 

The algorithm in \cite{Andoni08CACM} improves the existing research in \lsh and \plebf (PLEB) \cite{Indyk98STOC,Charikar02STOC}. 
PLEB was applied for noun clustering \cite{ravichandran05} and speech tasks \cite {JansenASRU11,JansenIS12}. 
Recent prior work on new variants of PLEB \cite{goyal12Flag} for distributional similarity can be seen as 
implementing a special case of Andoni and Indyk's \lsh algorithm. 

We first present four new variants of vanilla \lsh algorithm motivated by the technique of Multi-Probe LSH \cite{LvVLDB07}. 
A significant drawback of vanilla \lsh is that it requires a large number of hash tables in order to achieve good recall 
in finding nearest neighbors, making the algorithm memory intensive. The goal of  \mblshf 
is to get significantly better recall than the vanilla \lsh by using the same number of hash tables.
%The main idea behind \mblshf is to look up multiple buckets within a table that have a high probability 
%of containing the nearest neighbors of a query. We present a high-level idea behind the \mblshf algorithm; 
%for more details, the reader is referred to \cite{LvVLDB07}.


\subsection{Vanilla \lsh}
\label{sec:vlsh}
The \lsh algorithm relies on the existence of an family of locality sensitive hash functions. 
Let $H$ be a family of hash functions mapping 
 $\mathbb{R}^D$ to some universe S. For any two query terms $p$, $q$; we chose $h\in H$ uniformly at random; 
 and analyze the probability that $h(p) = h(q)$.  
 Suppose $d$ is a distance function (cosine distance for us), 
$R > 0$ be a distance threshold $R>0$ and $c>1$ be an approximation factor.   
Let $P_1, P_2 \in (0, 1)$ be two probability thresholds.  
The family $H$ of hash functions is called a $(R, cR, P_1, P_2)$ locality sensitive family if 
 it satisfies the following conditions:
\begin{enumerate}
\item If $d(p,q) \leq R$, then $Pr[h(p)=h(q)] \geq P_1$, 
\item and if $d(p,q) \geq cR$, then $Pr[h(p)=h(q)] \leq P_2$
\end{enumerate}
An \lsh family is generally interesting when  $P_1>P_2$. 
However, the difference between $P_1$ and $P_2$ can be very small. 
Given a family $H$ of hash functions with parameters $(R, cR, P_1,P_2)$, 
the \lsh algorithm is devised by amplifying the gap between 
the two probabilities $P_1$ and $P_2$ by concatenating several functions. 
In particular, \lsh algorithm concatenates $K$ hash functions
to create a new hash function $g(\cdot)$ as:
$g(q)=(h_1(q),h_2(q),\dots,h_K(q))$. 
A larger value of $K$ leads to larger 
gap between probabilites of collision between close neighbors (i.e. distance less than $R$) and neighbors that are far (i.e. 
distance more than $cR$); 
the corresponding probabilities being $P_1^{K}$ and $P_2^{K}$ respectively. 
This amplification ensures a high precision of the algorithm by  
making the probability of dissimilar queries having the same hash value very small.

In order to increase the recall of the \lsh algorithm, the algorithm of Andoni et al. then uses
$L$ hash tables, each constructed using a different $g_j(\cdot)$ function, where each 
$g_j(\cdot)$ is defined as $g_j(q)=(h_{1,j}(q),h_{2,j}(q),\dots,h_{K,j}(q)))$; $\forall 1 \leq j \leq  L$. 


%Recall = $1-(1-P_1^{K})^{L}$

\subsection{\lsh for Cosine Similarity}
\label{subsec:vlsh:cosine}
In this paper, we are interested in cosine similarity, and use the \lsh family defined by Charikar \shortcite{Charikar02STOC}. 
For two queries; $p,q \in \mathbb{R}^D$, the cosine similarity between them is $\left( \frac{p.q}{ \| p \| \| q \| }\right)$. 
The \lsh functions for cosine similarity is defined as follows: if $\alpha \in \mathbb{R}^D$ is a random vector, 
then a corresponding hash function $h_{\alpha}$ can be defined as $h_{\alpha}(p) = \mathrm{sign}(\alpha \cdot p)$.
Typically, a negative sign is represented as $0$ and positive sign as $1$, and indices of buckets in the hash tables (i.e. the
range of each $g_j$) are $K$ bit vectors. 
In order to create a random vector $\alpha$, we exploit the intuition in
\cite{Achlioptas03,LiH06} and sample each coordinate of $\alpha$ from $\{-1, +1\}$ with equal probability.
Practically, each coordinate of $\alpha$ is generated using a hash function that maps that coordinate
to $\{-1, +1\}$ (this is termed ``hashing trick'' in~\cite{weinberger09hashing}). 
%\footnote{Note, we represent our queries ($p$ and $q$) using only non-zero features.}  
This hashing trick is important and useful as we do not need to explicitly store the 
huge random projection matrix of size $D \times K \times L$.  

%To solve the above problem, we propose a distributed Hadoop framework based on Locality Sensitive Hashing. 
%Our framework has three main steps
%i) Generate $256$ bit signature for each query.  
%ii) Use the $256$ bit signature for each query to generate $L$ tables for each table with $K$ bit signature. 
%Next, generate query pairs which fall in the same table.
%iii) Compute cosine similarity between query terms which fall in the same table and return the query pairs  
%with similarity $\geq\tau$.  
%
Figure~\ref{alg:lsh} describes the algorithm for both creating the data structure, as well as for
querying it. In preprocessing step, the algorithm takes as input $N$ queries along with the associated feature vectors. 
In our setting, each query is represented using an extremely sparse and high dimensional feature vector
that is constructed as follows: for query $q$, we take all the webpages (urls) that any user has clicked on 
when querying for $q$. Using this representation, we then generate the 
$L$ different hash values for each query $q$, where each such hash value is again 
the concatenation of $K$ hash functions. These $L$ hash values per query are then used to create $L$ hash tables.
Since the width of the index of each bucket $K$, each hash table contains at most $2^K$ buckets. 
with each hash table containing at most $2^K$ buckets.
Each query term is then placed in their respective buckets for each of the $L$ hash tables.

In order to retrieve near neighbors, for each of the $M$ test queries, we first 
retrieve all the query terms which appear in the buckets associated with each of the $M$ test queries. 
Next, we compute cosine similarity between each of the retrieved terms 
and the input test queries and return all those queries as neighbors 
which are within a similarity threshold ($\tau$). 

In this work, we have implemented the above algorithm in a map-reduce setting (Hadoop). 
In Section \ref{subsec:eval:vanillaLSH}, we show that the map-reduce implmentation scales to hundreds of millions of queries.  

\begin{figure}
\fbox{
\begin{minipage}{2.5in}
{\bf Preprocessing:} Input is $N$ queries with their respective feature vectors.
\begin{itemize}
\item Select $L$ functions $g_j$, $j=1,2,\dots,L$, setting  $g_j(q)=(h_{1,j}(q),h_{2,j}(q),\dots,h_{K,j}(q))$, where 
$\{h_{i,j}(q), i\in [1, K], j\in [1,L]\}$ are chosen at random from the \lsh family.
\item Construct $L$ hash tables, $\forall 1 \leq j \leq  L$. 
All queries with similar $g_j$ functions ($\forall 1 \leq j \leq  L$) are placed in the same bucket.    
\end{itemize}

{\bf Query:} $M$ test queries. Let $q$ denote a test query. 
\begin{itemize}
\item For each $j=1,2,\dots,L$
\begin{itemize}
\item Retrieve all the queries from the bucket with $g_j(q)$ function as the index of the bucket 
\item Compute cosine similarity between query q and all the retrieved queries. Return all the queries which have similarity affinity within a similarity threshold ($\tau$). 
\end{itemize}

\end{itemize}
\end{minipage}
}
\caption{\lshf Algorithm}
\label{alg:lsh}
\end{figure}

\subsection{Reusing Hash Functions}
\label{subsec:resuseHash}
In Section \ref{subsec:vlsh:cosine}, we showed that vanilla \lsh requires $L\times K$ hash functions. 
However generating hash functions is computationally expensive as 
it takes time to read all features and 
evaluate hash functions over all those features to generate a single bit. 
To minimize the number of hash functions computations, 
we use a trick from Andoni and Indyk \shortcite{Andoni08CACM} 
in which hash functions are reused to generate $L$ tables. $K$ is assumed to be even and $R \approx	\sqrt L$. 
We generate $f(q)=(h_1(q),h_2(q),\dots,h_{K/2}(q)))$ of length $k/2$. 
Next, we define $g(q)=(f_a,f_b)$, where $1\leq a < b \leq R$. This scheme generates $L= \frac{R (R -1)}{2}$. 
This scheme requires O($K  \sqrt L$) hash functions, instead of O($KL$).    
For rest of this paper, we use the above trick to generate $L$ hash tables with $K$-widith bit bucket-ids. 
\begin{table}
\centering
{
\small \addtolength{\tabcolsep}{-4.5pt}
\begin{tabular}{ll}
\hline
\hline
 \textbf{Symbol} &  \textbf{Description} \\
\hline 
$N$ & \# of query terms \\
$D$ & \# of features i.e. all clicked unique urls \\
\hline
\multirow{3}{*}{$K$} & \# of hash functions concatenated together \\ 
   & $g(q)=(h_1(q),h_2(q),\dots,h_k(q)))$ \\ 
	 & to generate the index of a table \\
\hline
 \multirow{2}{*}{$L$} & \# of tables generated independently \\ 
	  &  with $g_j(q)$ index, $\forall 1 \leq j \leq  L$  \\
\hline
$F$ & \# of bits flipped, $\forall 1 \leq j \leq  L$   \\
$\tau$ & $\tau$ threshold \\
Recall & fraction of similar candidates retrieved \\
Comparisons & Avg \# of pairwise comparisons per query  \\
\hline 
\end{tabular}
\caption{\footnotesize{Major Notations}}
}
\label{tab:notation}
\end{table}


\subsection{Multi Probe \lsh}
\label{sec:mblsh}
As we discussed in Section \ref{subsec:resuseHash} that generating hash functions can be computationally expensive.  
That is exactly one of the significant drawbacks of vanilla \lsh as it requires a large number of hash tables (large $L$) 
in order to achieve good recall in finding nearest neighbors.
Here, We describe four new variants of vanilla \lsh algorithm motivated by the research on \mblshf \cite{LvVLDB07}. 
The goal of  \mblshf is to get significantly better recall than the vanilla \lsh by using 
the same number of hash tables. The main concept behind \mblshf is to look 
up multiple buckets within a table that have a high probability of containing the nearest neighbors of a query. 
Note, eventhough vanila \lsh and \mblshf both have same number of hash tables ($L$); 
\mblshf requires more number of probes as it searches in multiple buckets within a table. 
The main advantage of searching in multiple buckets over generating more number of tables 
is that it takes more memory and time to generate more tables in pre-processing. 


The original \mblshf algorithm is developed for Euclidean distance. 
If interested in details about \mblshf algorithm for Euclidean distance, the reader is referred to \cite{LvVLDB07}. 
Here, we propose four variants of \mblshf for cosine similarity:
\begin{itemize}
\item {\bf \rflipq:} The first variant is our baseline: first we compute the initial \lsh of a query q, which gives the $L$ bucket ids.  
Next, we create alternate bucket ids by taking each of the $L$ bucket ids and then creating alternate candidate buckets by flipping a 
set of coordinates randomly in the \lsh of just the query.  
\item {\bf \rflipb:} The second variant is another baseline similar to the previous one. Instead of just flipping the bits for only the query, 
here we flip bits for both the query and all the queries in the database. 
\item {\bf \dflipq:} The third variant is intelligent version of first variant. Instead of randomly flipping some coordinates, 
we devisely select a set of coordinates based on the distance of query term from the random hyperplane (hash function) 
that was used in to create this coordinate. 
The distance of query term from the random hyperplane is the absolute value which we get before applying the sign function on it (see Section \ref{subsec:vlsh:cosine}). 
\item {\bf \dflipb:} The fourth version is similar to third one, however 
here we flip bits for both the query and the queries in the database (intelligent version of second baseline). 
\end{itemize}


%we have experimented with is the following variant of Multi-probe \lsh:  first we compute the initial \lsh of a query q, which gives the $L$ bucket ids. 

%We have experimented with two different methods to choose which coordinate to flip: randomly choosing a specified number of coordinates or choosing the set of coordinates based on the distance of q from the random hyperplane that was used in to create this coordinate.  
%{\rflip \lsh}
%Random Flips

%We experimented with flipping the bits in just the query and both the query and the database of all queries.  
%1. Generate less number of hash functions
%2. Have to compute similarity over less number of data pairs

%Advantages of Flipping bits compared to increasing L:
%1. Increasing $L$ means generating more random projection bits that is both time and memory intensive.   
%Note, we are already using the trick mentioned in Original \lsh to decrease
%the number of tables. Our implementation requires O(sqrt(L)) tables.
%2. Probing in more tables for vanilla \lsh, hence we need to read data from the disk (more disk read operations.). While Flipping bits, data is
%already in memory, and the given data needs to be flipped.
%3. Disadvantage of flipping is we need to store random projection distances to perform flipping (which takes space).


%\dflip \lsh
%Based on random projection distance 
