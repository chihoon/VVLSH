
 \section{Introduction}
Every day, 
hundreds of millions of users visit commercial search engines to pose
queries on topics of their interest.  
Such queries are typically just a few key words intended to specify
the topic that the user has in mind. 
To provide users with a high quality service, 
search engines such as Bing, Google, and Yahoo require
intelligent analysis to realize users' implicit intents.
The key resource that they have to help tease out what is meant is
their large history of requests, in the form of large scale query logs.  
A key primitive in 
%One of the key interesting tasks involved in 
learning users' implicit intents is the computation of 
nearest neighbors (queries) for a user given query. 
Computing nearest neighbors is useful for 
many search-related problems on the Web and the Mobile such as 
finding related queries \cite{Jones06WWW,Jain11SIGIR,Song12WSDM}, 
finding near-duplicate queries \cite{}, spelling correction \cite{},  
paraphrasing \cite{petrovicNAACL12,ganitkevitch13Paraphrase}, 
and diversifying search results \cite{Song11SIGIR}.  

In this paper, we focus on the problem of finding nearest neighbors
for a given query from very large scale query logs available from a commercial search engine. 
%In order to understand the implicit users' intent, each query is initially represented in a high dimensional feature space,
%where each dimension corresponds to a clicked url.
Given the importance of this question, it is important to design
algorithms that can scale to many queries over huge logs, and allow
online and offline computation. 
However, computing nearest neighbors of a query 
%from large-scale query logs is a non-trivial task 
can be very costly. 
Naive solutions that involve a linear search of the set of possibilities
are simply infeasible in these settings. 
%due to the computational complexity among hundreds of millions of  queries.
%The reason being conducting pairwise computations between a given query and all the queries in the dataset involves 
%linear number of comparisons in the query dataset size. For finding neighbors for hundreds of millions of queries, 
%it means doing petabytes of comparisons; that is computationally challenging or infeasible even in a distributed setting (such as Hadoop). 
Even though distributed computing environments such as Hadoop
make it feasible to store and search large data sets in parallel, 
%Note that even in a distributed computing environment such as Hadoop,
%the
the naive pairwise computation is still infeasible. 
The reason is that the total amount of work performed is still huge,
and simply throwing more resources at the problem is not effective. 
Given a log of hundreds of millions queries, most are ``far'' from a
query of interest, and we should aim to avoid doing many ``useless''
comparisons that confirm that queries are indeed far from it. 
% from our experiments  using 2000 random queries to attempt to retrieve neighbors from the query logs consisting of hundreds of millions of queries. In other words, even though the big \textit{O} notation is denoted as linear, \textit{2000*N} comparisons are not feasible when \textit{N} reaches to hundreds of millions.

In order to address the computational challenge, this paper aims to find nearest neighbors by doing a 
\emph{small} number of comparisons---that is, sublinear in the dataset size---instead of brute force linear search. 
In addition to \emph{small} number of comparisons, we aim to 
retrieve neighboring candidates with  $100\%$ precision and high recall.
It is important that false positive rate (ratio of  ``incorrectly'' identifying queries as neighbors) is penalized more severely than false negative (ratio of missing ``true'' neighbors).

In the case of seeking exact matches for queries, effective solutions
are based on storing values in a hash table and mapping in via hash
functions. 
The approximate generalization of this approach is the framework of
\lshf, where queries are more likely to collide
under the hash function if they are more alike, and less likely to collide
if they are less alike. 
The methods we propose in this paper meet our criteria by extending \lshf 
\cite{Indyk98STOC,Charikar02STOC,Andoni06FOCS,Andoni08CACM}  in novel ways. 
In particular, we apply the framework within a distributed 
system, Hadoop, and take advantage of its distributed computing power.

Our work makes the following contributions: 
\begin{enumerate}
\item We begin with a ``vanilla'' \lsh algorithm based on the seminal
  research of Andoni and Indyk  \shortcite{Andoni08CACM}.
 To best of our knowledge, this is the first paper that applies this algorithm to NLP applications. 
\item We propose four novel variants of vanilla LSH motivated by the
  research on Multi-Probe LSH \cite{LvVLDB07}.  We show that two of
  our variants achieve significantly better recall than the vanilla
  \lsh by using the same number of hash tables. The main idea behind
  these variants is to intelligently probe multiple ``nearby'' buckets within a table that have a high probability of containing the nearest neighbors of a query.  
\item We present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial large-scale query logs in sublinear time. % Can we claim something more specific wrt time (e.g. 0(N logN))?
%: Random Flipping and Query-Driven Flipping. 
\item  We show that the applicability of our system on two real-world
  applications: finding related queries and removing (near) duplicate
  queries.
The system outperforms competitor methods, and is currently being
implemented for production use within a large search provider. 
%(Increasing  direct display coverage is related to finding related queries) and ??.
\end{enumerate}

%We are interested in the following setting: given a large set of 
%points $q_1,q_2,\dots,q_n$, each of them is a search-query. 

%Spelling {grouping mis-spellings in same group}
%Similar entities {LOTR,}
%Finding related queries
