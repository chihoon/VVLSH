
 \section{Introduction}
Millions of users visit commercial search engines and ``query'' 
their interests. To provide users with high quality of services, 
search engines such as Bing, Google, and Yahoo require
intelligent analysis to realize users' implicit intents, in particular taking
advantage of large scaled query logs.  One of the key
interesting tasks involved in learning users' implicit intents often involves 
computing nearest neighbors (queries) for a user given query. 
Computing nearest neighbors is useful for 
many search-related problems on the Web and the Mobile such as 
finding related queries \cite{Jones06WWW,Jain11SIGIR,Song12WSDM}, 
finding near-duplicate queries \cite{}, spelling correction \cite{},  
, paraphrasing \cite{petrovicNAACL12,ganitkevitch13Paraphrase}, 
and diversifying search results \cite{Song11SIGIR}.  

In this research, we focus on finding nearest neighbors for a given query
from very large scaled query logs available from a commercial search engine. 
%In order to understand the implicit users' intent, each query is initially represented in a high dimensional feature space,
%where each dimension corresponds to a clicked url.
However, computing nearest neighbors of a query 
from large-scale query logs is a non-trivial task due to the computational complexity among hundreds of millions of  queries.
%The reason being conducting pairwise computations between a given query and all the queries in the dataset involves 
%linear number of comparisons in the query dataset size. For finding neighbors for hundreds of millions of queries, 
%it means doing petabytes of comparisons; that is computationally challenging or infeasible even in a distributed setting (such as Hadoop). 
Note that even in a distributed computing environment such as Hadoop, the naive pairwise computation is not feasible from our experiments  using 2000 random queries to attempt to retrieve neighbors from the query logs consisting of hundreds of millions of queries. In other words, even though the big \textit{O} notation is denoted as linear, \textit{2000*N} comparisons are not feasible when \textit{N} reaches to hundreds of millions.

In order to address the computational challenge, this paper exploits to find nearest neighbors by doing a 
\emph{small} number of comparisons -- that is, sublinear in dataset size -- instead of brute force linear search. 
In addition to \emph{small} number of comparisons, we aim to 
retrieve neighboring candidates with a $100\%$ precision and a large recall.
It is important that false positive rate (ratio of  ``incorrectly'' identifying queries as neighbors) is to be penalized more severely than false negative (ratio of missing ``true'' neighbors).

The methods we propose in this paper meet all these criterions by extending existing research in \lshf 
\cite{Indyk98STOC,Charikar02STOC,Andoni06FOCS,Andoni08CACM}  to novel variants. 
In particular, we develop the framework of  the variants on a distributed 
system, Hadoop by taking advantage of its distributed computing power.

Our work includes following contributions: 
\begin{enumerate}
\item We present vanilla \lsh algorithm based on the seminal research of Andoni and Indyk  \shortcite{Andoni08CACM}. To best of our knowledge, this is the first paper that applies this algorithm to NLP applications. 
\item We propose four novel variants of vanilla LSH motivated by the research on Multi-Probe LSH \cite{LvVLDB07}.  We show that two of our variants achieve significantly better recall than the vanilla \lsh by using the same number of hash tables. The main idea behind these variants is to intelligently look up multiple buckets within a table that have a high probability of containing the nearest neighbors of a query.  
\item We present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial large-scale query logs in sublinear time. % Can we claim something more specific wrt time (e.g. 0(N logN))?
%: Random Flipping and Query-Driven Flipping. 
\item  We show that the applicability of our system on two real-world applications such as finding related queries and removing duplicated queries.
%(Increasing  direct display coverage is related to finding related queries) and ??.
\end{enumerate}

%We are interested in the following setting: given a large set of 
%points $q_1,q_2,\dots,q_n$, each of them is a search-query. 

%Spelling {grouping mis-spellings in same group}
%Similar entities {LOTR,}
%Finding related queries
