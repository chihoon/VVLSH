
\section{Experiments}
We evaluate our  distributed large-scale approximate near neighbor framework by conducting 
several experiments on publicly available query logs as well as a 
large-scale query log collected from a commercial search engine. 


\begin{table}
\centering
\begin{tabular}{|c|r|r|}
\hline
Data & $N$ & $D$  \\ 
\hline
\aol &  $0.3 \times 10^6$  & $0.7 \times 10^6$ \\
\dataA & $6 \times 10^6$  & $66 \times 10^6$ \\
\dataB & $62 \times 10^6$  & $464 \times 10^6$ \\
\dataC &  $617 \times 10^6$  & $ 2.4 \times 10^9$  \\
\hline 
 \end{tabular}
\caption{\footnotesize{Query-logs statistics}}
\label{tab:data}
\end{table}

\subsection{Data}
The public dataset that we demonstrate results on is adapted from the query logs of the AOL search engine \cite{Pass06}  (\aol dataset). 
Moreover, we show results on a large query log (hundreds of millions of queries) sampled from a commercial search engine. 
We started by collecting a dataset over multiple days, and containing $N=600$ million unique queries: this does not represent
an exhaustive set of queries posed to the search engine. We then created multiple datasets from this corpus by subsampling it at various rates.
\dataA represents a $1\%$ sample of queries from the above corpus, \dataB represents a $10\%$ sample and \dataC represents the entire corpus. 
For each query, a high dimensional and sparse, feature vector was created over the domain of urls (the size of the domain is a few billion).   
The feature vector of a query $q$ contains the  webpages (urls) that have received a user click for this search query $q$. The weight of 
an url feature for a query $q$ depends on the click through rate of this url for the query $q$.  
As a pre-processing step, we remove all the queries that have less than or equal to five clicked urls. 
Table \ref{tab:data} summarizes the statistics of our query-log datasets. 

\paragraph{Test Data}: We conduct all the experiments using $2000$ random queries sampled from the query logs as the test set of queries. 
For evaluation, we compute the true similar candidates for all the $2000$ test queries by calculating cosine similarity 
between each test query and all the queries in the rest of the dataset. For most of the experiments, we set the similarity threshold  $\tau=0.7$, 
meaning that the algorithm needs to retrieve candidates that have cosine similarity larger than or equal to $0.7$.


\subsection{Evaluation Metrics}
We use two metrics for evaluation: recall and number of comparisons. 
Recall of an LSH algorithm is the fraction of \emph{true} similar candidates (found using candidates returned by computing exact cosine similarity) 
that is retrieved by the LSH algorithm.  
The number of comparisons performed by an algorithm is computed as the average number of pairwise comparisons that is done per test query. 
The goal of this paper is to maximize recall and minimize the number of comparisons.

\subsection{Evaluating Vanilla LSH} 
\label{subsec:eval:vanillaLSH}



\begin{table}
\centering
\begin{tabular}{|c|rc|rc|}
\hline
\multirow{2}{*}{$\tau$}  & \multicolumn{2}{c|}{\small \aol} & \multicolumn{2}{c|}{\small \dataA} \\
 & {\small Comparisons} & {\small Recall} & {\small Comparisons} & {\small Recall}  \\
\hline
0.7 & \multirow{3}{*} {57}  & .63  & \multirow{3}{*} {1052} & .67 \\
0.8 &   & .84  &  & .81 \\
0.9 &   & .98  &  & .96 \\
\hline 
 \end{tabular}
\caption{\footnotesize{Varying $\tau$ with fixed $K=16$ and $L=10$ on \aol and \dataA.}}
\label{tab:varyTau}
\end{table}

In the first experiment, we vary the similarity threshold parameter $\tau$ to be in $\{0.7,0.8,0.9\}$ 
while fixing $K=16$ and $L=10$ for the \aol and \dataA datasets. 
Table~\ref{tab:varyTau} shows that as expected, finding near-duplicates, when $\tau=0.9$, 
is easier than finding nearest neighbors when $\tau=0.7$. For, rest of this paper, 
we fix $\tau=0.7$ as we are interested in both near-duplicates and nearest-neighbors for our test queries. 

In the second experiment, we vary $L$ to be in $\{1,10,28,55\}$ while fixing $K=16$ on the \aol and \dataA datasets. 
Recall that $L$ denotes the number of hash tables and $K$ is the width of the index of the buckets in the table. 
Increasing $K$ results in increase of the precision by reducing the false positive candidates, but 
the $L$ also need to be correspondingly increased in order the maintain a good recall
(i.e. reduce false negatives). 
Table \ref{tab:varyL} shows that increasing $L$ leads to better recall, 
but at the expense of performing more comparisons on both the datasets. 
In addition, having a large $L$ 
means generating large number of random projection bits and hash tables which is 
both time and memory intensive. Hence, we fix $L=10$, 
which leads to a reasonable recall with a tolerable number of comparisons. 

\begin{table}
\centering
\begin{tabular}{|c|rc|rc|}
\hline
\multirow{2}{*}{L} & \multicolumn{2}{c|}{\small \aol} & \multicolumn{2}{c|}{\small \dataA} \\
 & {\small Comparisons} & {\small Recall} & {\small Comparisons} & {\small Recall}  \\
\hline
1 & 7  & .28 & 106 & .36 \\
 \rowcolor[gray]{0.9} 10  &  57 & .63 & 1052 & .67 \\
28 &  152 & .77 & 2908 & .78 \\
55 &  297 & .89 & 5648 & .84 \\
\hline 
 \end{tabular}
\caption{\footnotesize{Varying $L$ with fixed $K=16$ on \aol and \dataA with $\tau=0.7$.}}
\label{tab:varyL}
\end{table}
In the third experiment, we vary $K$ to be in $\{4,8,16\}$ while fixing $L=10$ on \aol and \dataA datasets. 
As expected, Table \ref{tab:varyK} shows that increasing $K$ reduces the number of 
comparisons and worsens recall on both the datasets. This is intuitive as  
the larger value of $K$ leads to larger 
gap between probabilities of collision between close queries 
and far queries (see Section \ref{sec:vlsh}). 
Hence, we fix $K=16$ to have few number of comparisons.   


\begin{table}
\centering
%\begin{tabular}{|c|r|c|}
\begin{tabular}{|c|rc|rc|}
\hline
%$K$ & Comparisons & Recall \\
\multirow{2}{*}{K} & \multicolumn{2}{c|}{\small \aol} & \multicolumn{2}{c|}{\small \dataA} \\
 &{\small Comparisons} & {\small Recall} & {\small Comparisons} & {\small Recall}   \\
\hline
4  &  112,347 & .98 & 2,29,2670 & .96 \\
8 &  11,008 & .90 & 221,132 & .88 \\
\rowcolor[gray]{0.9}16 &  57 & .63 & 1,052 & .67 \\
\hline 
 \end{tabular}
\caption{\footnotesize{Varying $K$ with fixed $L=10$ on \aol with $\tau=0.7$.}}
\label{tab:varyK}
\end{table}

In the fourth experiment, we fix $L=10$ and $K=16$, values that were found to be reasonable using the previous experiments, 
and then increase the size of training data. 
Table~\ref{tab:varyData} demonstrates that as we increase the training data size, 
the number of comparisons done by the algorithm also increase. 
This result indicates that $K$ needs to be tuned with respect to a specific dataset, 
as a larger $K$ will reduce the probability of dissimilar 
queries falling within the same bucket. 
$K$ and $L$ can be tuned by randomly sampling small set of queries. In this paper, we randomly select $2000$ queries to tune parameter $K$.

Table \ref{tab:bestLSH}, containing the result of our fifth experiment, shows the best $K$ and $L$ parameter settings on datasets with different 
sizes.\footnote{Recall on \dataC the precision/recall cannot be computed, as it was computationally intensive to find exact similar neighbors.} 
On our biggest dataset of $600$ million queries, we set $K=24$ and $L=10$. These parameter settings require on an average only 
$464$ comparisons to find approximate nearest neighbors compared to exact cosine similarity that involves 
brute force search over all $600$ million queries in the dataset.  




\begin{table}
\centering
\begin{tabular}{|c|r|c|}
\hline
Data & Comparisons & Recall \\
\hline
\aol & 57  & .63 \\
\dataA  &  1,052 & .67 \\
\dataB    & 10,515 & .64 \\
\dataC    & 105,126 & - \\
\hline 
 \end{tabular}
\caption{\footnotesize{Fixed $K=16$  and $L=10$ on different sized datasets with $\tau=0.7$.}}
\label{tab:varyData}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|r|c|}
\hline
{\small Data} & {\small Comparisons} & {\small Recall} \\
\hline
\aol  ($K=16$) & 57  & .63 \\
\dataA ($K=16$)  &  1,052 & .67 \\
\dataB ($K=20$)   & 695 & .53 \\
\dataC ($K=24$)   & 464 & - \\
\hline 
 \end{tabular}
\caption{\footnotesize{Best parameter settings (Based on minimizing number of comparisons and maximizing recall) of $K$ with fixed $L=10$ on different sized datasets.}}
\label{tab:bestLSH}
\end{table}


\begin{table}
\centering
{
\small \addtolength{\tabcolsep}{-4.5pt}
\begin{tabular}{|c|cc|cc|} 
\hline
Method & \multicolumn{2}{c|}{\rflipq} & \multicolumn{2}{c|}{\dflipq}   \\
\hline
$F$ & Comparisons & Recall  & Comparisons & Recall \\
\hline
1 & 108 & .65 & 106 & .72 \\
2 & 159 & .66 &  {\it155} & {\bf .75}  \\
5 &  311 & .70 &  303 &  .79  \\
10 & 557  & .75 &  552 &  .81  \\
16 & 839   & .82 &  839 &  .82  \\
\hline 
 \end{tabular}
 }
\caption{\footnotesize{Flipping the bits in the query only with $K=16$ and $L=10$ on \aol  with $\tau=0.7$.}}
\label{tab:query:aol}
\end{table}


\begin{table}
\centering
{
\small \addtolength{\tabcolsep}{-4.5pt}
\begin{tabular}{|c|cc|cc|} 
\hline
Method & \multicolumn{2}{c|}{\rflipb} & \multicolumn{2}{c|}{\dflipb}   \\
\hline
$F$ & Comparisons & Recall  & Comparisons & Recall \\
\hline
1 & 204 & .71 & 192 & .80 \\
 2 & 433 & .73    &  {\it 405} & {\bf .86}  \\
5 &  1557 & .86  & 1475 & .93     \\
10 &  4138 & .94 & 4059 & .96     \\
16 &  5922 & .96 & 5922 & .96     \\
\hline 
 \end{tabular}
 }
\caption{\footnotesize{Flipping the bits in both the query and the database with $K=16$ and $L=10$ on \aol  with $\tau=0.7$.}}
\label{tab:both:aol}
\end{table}


\begin{table}
\centering
{
\small \addtolength{\tabcolsep}{-4.5pt}
\begin{tabular}{|c|cc|cc|} 
\hline
Method & \multicolumn{2}{c|}{\dflipq} & \multicolumn{2}{c|}{\dflipb}   \\
\hline
{\small Data} & {\small Comps.} & {\small Recall}  & {\small Comps.} & {\small Recall} \\
\hline
{\small \aol ($K=16$)} & 155 & .75 & 405 & .86 \\
{\small \dataA ($K=16$)} & 2980 & .76  & 7904  & .84   \\
{\small \dataB ($K=20$)} & 1954  & .64 & 5242  & .72     \\
{\small \dataC ($K=24$)} & 1280  & -  & 3427 & -      \\
\hline 
 \end{tabular}
 }
\caption{\footnotesize{Best parameter settings (Based on minimizing number of comparisons and maximizing recall) of $K$ with fixed $L=10$ and $F=2$ on different sized datasets with $\tau=0.7$.}}
\label{tab:dataset:final}
\end{table}


\begin{table*}[t]
\centering
\footnotesize \addtolength{\tabcolsep}{-0.5pt} \addtolength{\tabcolsep}{-0.6pt}
{

\begin{tabular}{|c!{\vrule width 1.3pt}c!{\vrule width 1.3pt}c!{\vrule width 1.3pt}c!{\vrule width 1.3pt}c|}
\hline
\textbf{how lbs in a ton}&\textbf{coldwell banker baileys harbor }&\textbf{michaels} &\textbf{trumbull ct weather} \\ %&\textbf{E} \\
%\textbf{how lbs in a ton}&\textbf{coldwell banker baileys harbor }&\textbf{michaels}&\textbf{D}&\textbf{E} \\
\hline
how much lbs is a ton & coldwell banker sturgeon bay wi & maichaels & trumbull ct weather forecast \\
number of pounds in a ton & coldwell banker door county & machaels & weather in trumbull ct  \\
how many lb are in a ton & door county wi mls listings & mechaels   & weather in trumbull ct 06611 \\
How many pounds are in a ton? & door county realtors sturgeon bay & miachaels & trumbull weather forecast \\
how many pounds in a ton & DOOR CTY REAL & michaeils & trumbull ct 06611 \\
1 short ton equals how many pounds & door county coldwell banker & michaelos & trumbull weather ct \\   
how many lbs in a ton? & door realty & michaeks & trumbull ct weather report \\
how many pounds in a ton? & coldwell banker door county horizons & michaeels & trumbull connecticut weather \\ 
How many pounds are in a ton & door county coldwell banker real estate & michaelas & weather 06611  \\
how many lb in a ton & coldwell banker door county wisconsin & michae;ls  & weather trumbull ct  \\
\hline
\end{tabular}
\caption{\footnotesize{Sample $10$ similar neighbors returned by \dflipb with $L=10$, $K=24$, and $F=2$ on \dataC dataset.}}
\label{tab:lists}
}
\end{table*}


\subsection{Evaluating Multi-Probe LSH}
\label{subsec:eval:multiProbeLSH}
In the first experiment, we compare flipping the bits in query only. 
We evaluate two approaches: \rflipq and \dflipq. We can make several observations from Table \ref{tab:query:aol}: 1) As expected, increasing the number of flips improve recall at expense of more comparisons for both \dflipq and \rflipq. 2) The last row of the Table \ref{tab:query:aol} shows that once we flip all the $K$ bits ($F=16$),  \dflipq and \rflipq converge to the same algorithm.  3) Our results show that \dflipq has significantly better recall than \rflipq with similar number of comparisons. In second row of the table with $F=2$, \dflipq has nine points better recall than \rflipq.  

In the second experiment, we compare flipping the bits in both query and the database. 
We can make similar observations from Table \ref{tab:both:aol} as made in the first experiment. 
In the second row of the Table with $F=2$, \dflipb has thirteen points better recall than \rflipb with similar number of comparisons. 
Comparing across second row of  Table \ref{tab:query:aol} and \ref{tab:both:aol} shows that flipping the bits in both query and the database has better 
recall at the expense of more comparisons. This is expected as flipping both means that we can find queries at distance two (one flip in query, one flip in database), 
hence more queries in each table when we probe. Note, we also compared distance based flipping with random flipping on different sized data-sets, 
and found that distance based flipping is always significantly better in terms of recall as compared to random flipping.\footnote{Due to limited space, we omit those results.} 

We select $F=2$ as the best parameter setting with goal of maximizing recall by restricting comparisons to minimum. For better recall at expense of more comparisons, $F=5$ can also be selected. However, results in Table \ref{tab:query:aol} and \ref{tab:both:aol} indicate that $F>5$ does \emph{not} increase recall significantly and at the same time leads to more number of comparisons.   

In the third experiment, we show the results of both variants of distance based Multi Probe that is \dflipq and \dflipb on different sized datasets. 
Table \ref{tab:dataset:final} shows the results with parameter $L=10$, $F=2$, and data-size dependent $K$ (same settings of $K$ for different sized datasets
as used in the last experiment of Section \ref{subsec:eval:vanillaLSH}).  As observed in the last experiment, flipping bits in both query and the database
is significantly better in terms of recall with more number of comparisons. The second and third row of the table respectively 
shows that flipping bits in both query and the database has eight points better recall on both \dataA and \dataB 
datasets. With the goal of maximizing recall with some extra comparisons, we select \dflipb as our final algorithm. 
\dflipb maximizes recall with small number of tables and comparisons. 
On our entire corpus (\dataC) with hundreds of millions of queries, 
\dflipb only requires $3,427$ comparisons compared to hundreds of millions of comparisons by exact brute force algorithm.  
\dflipb returns $9$ neighbors on average per given query; averaged over $2000$ random test 
queries.\footnote{As many queries can be long, hence such queries only have few neighbors.} 


In the fourth experiment, Table \ref{tab:lists} shows the qualitative results for some arbitrary queries. These results are found by applying our
system (\dflipb with parameters $L=10$, $K=24$, and $F=2$ ) on \dataC. The second column in Table \ref{tab:lists} shows that the returned approximate 
similar neighbors can be useful in finding related queries \cite{Jones06WWW,Jain11SIGIR}. 
The third column shows an example, where we can find several popular spelling errors. 
The first and last column show examples of near-duplicate queries \cite{leeCIKM11}.  

%different variants of a query, where user intends to find out the ``weather in trumbull ct''. 
%Modern search engines provide a direct display with respect to ``weather'' related queries. 
%Hence, if we don't have enough evidence about a specific query being related to ``weather'', we can use queries approximately similar to it to infer that 
%if this query is about ``weather'' or not.  

% paraphrasing \cite{ganitkevitch13Paraphrase}
% maybe include deduplication results




