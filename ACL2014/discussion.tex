\section{Discussion and Conclusion}
\label{sec:discuss}
In this section, we highlight several applications that take significant advantage of the approximate \dflipb algorithm presented in this paper.
One of the interesting applications of the near-neighbor finding is to understand specific intents behind the user query. 
Given a user's query, Bing, Google, and Yahoo often delivers direct display results that summarize expected contents of the query. 
For instance, when a query ``f stock price'' is issued  to search engines, the quick summary of the stock quote with a chart is delivered 
to the user as the part of the search engine result page. 
Such direct display results are expected to reduce the number of unnecessary clicks by providing the user with the appropriate content early on. 
However, when the query "f today closing price" is issued to search engines, the three major search engines 
fail to deliver the same direct display experience to the user query, although its query intent is strongly related to ``f stock price''. 
By employing an algorithm similar to \dflipb, we can build a synonym database, 
which will help trigger the same direct display among related queries.

Yet another application is removing duplicated instances in a result set. 
When a query set is retrieved from a repository and presented to users, it is important to remove similar queries as user experiences 
would be distracted from ``duplicated results.'' 
That is, given a set of query terms, we can apply  \dflipb algorithm to build a lookup table of near-duplicates 
to find ``duplicated query terms'' efficiently. As ``duplicateness'' of among query terms typically requires ``higher'' degree of similarity (relatively easier problem) than ``relatedness'', we can tune parameters ($K, L, F$) based on a specific $\tau$ (e.g $\tau=0.9$) from training samples.  The fourth column in Table~\ref{tab:lists} illustrates several duplicate examples -- ``trumbull weather ct'' and ``weather in trumbull ct''.

We first described the existing ``vanilla'' \lsh algorithm. It is the first paper according to 
best of our knowledge that applies this algorithm to NLP applications. 
Next, we poposed four novel variants of vanilla \lsh. Two of our variants achieve 
significantly better recall than the vanilla LSH by using the same number of hash tables. 
We present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial large-scale query logs (consisting of hundreds of millions of queries) in sublinear time.
