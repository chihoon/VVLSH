\section{Discussion}
\label{sec:discuss}
In this section, we highlight several applications that can take significant advantage of the approximate \dflipb algorithm presented in this paper.
One of the interesting applications of the near-neighbor finding is to understand specific intents behind the user query. 
Given a user's query, Bing, Google, and Yahoo often delivers direct display results that summarize expected contents of the query. 
For instance, when a query ``f stock price'' is issued  to search engines, the quick summary of the stock quote with a chart is delivered 
to the user as the part of the search engine result page. 
Such direct display results are expected to reduce the number of unnecessary clicks by providing the user with the appropriate content early on. 
However, when the query "f today closing price" is issued to search engines, the three major search engines 
fail to deliver the same direct display experience to the user query, although its query intent is strongly related to ``f stock price''. 
By employing an algorithm similar to \dflipb, we can build a synonym database, 
which will help trigger the same direct display among related queries.

Yet another application is removing duplicated instances in a set of suggested results. 
When a query set is retrieved from a repository and presented to users, it is important to remove similar queries
from the set so that the user is not distracted by duplicated results. Given a set of
we can apply  \dflipb algorithm to build a lookup table of near-duplicates 
in order to find the ``duplicated query terms'' efficiently. 
As ``near-duplicates''  among query terms typically requires ``higher'' degree of similarity 
(relatively easier problem) than ``relatedness'', 
we can tune parameters ($K, L, F$) based on a specific $\tau$ (e.g $\tau=0.9$) from training samples. 
The fourth column in Table~\ref{tab:lists} illustrates several duplicate examples-- ``trumbull weather ct'' and ``weather in trumbull ct''.

\section{Conclusion}
\label{sec:conclusion}
To the best of our knowledge, this is the first paper that applies
the vanilla \lsh algorithm of Andoni et al. to the NLP domain. 
We also proposed four novel variants of \lsh that are aimed to reduce the number 
of hash tables used. Two of our variants achieve 
significantly better recall than the vanilla LSH by using the same number of hash tables. 
We also present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial 
large-scale query logs in sublinear time. On our entire corpus (\dataC) with hundreds of millions of queries, 
\dflipb only requires 3,427 comparisons compared to hundreds of millions of comparisons by exact brute force algorithm.
In future, we will extend our \lsh framework to several large-scale NLP and social media applications.  

