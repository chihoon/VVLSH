

\begin{abstract}
Many applications from various domains such as web search, mobile
browsing, and NLP rely on finding the nearest neighbors of a given
text string from a large database.
Due to the very large scale of data involved 
(e.g., users' queries from commercial search engines),
computing nearest neighbors is the best way to 
%often required to
understand users' intents. 
However, it is a non-trivial
task as the computational complexity grow significantly with the number of queries.
To address this challenge, we exploit 
\lshf (a.k.a, \lsh) methods and propose 
novel varaints in a distributed computing environment (specifically, Hadoop). 
We identify several optimizations which improve performance, suitable
for deployment in very large scale settings. 
The experimental results demonstrate our 
proposed variants of \lsh achieve the robust performance with better
recall compared with ``vanilla'' \lsh,  even
when using the same space determined by the number of hash tables.

%Many Web, Mobile, and NLP applications 
%rely on finding nearest neighbors 
%for a given query from a commercial large-scale query logs . 
%Due to the large scale volume of user queries, 
%computing nearest neighbors from all queries using brute force linear 
%search is a computationally time intensive task. 
%To challenge the aforementioned problem, we exploit existing 
%\lshf (a.k.a, \lsh) methods and propose 
%their novel variants in distributed setting (e.g. Hadoop). 
%Our experimental results show that two of our 
%new variants of \lsh get significantly better recall than vanilla \lsh by 
%using the same number of hash tables. 
\end{abstract}

